Что есть:

    программа скачивает страницу из сети, вынимает из неё все ссылки,
    определяет, по каким из них нужно перейти дальше, скачивает их, и т.д.
    если похоже, что ссылка ведёт на книжку, то она выводится в ответ
    - поддерживается стандарт robots.txt (файл, содержащийся на многих серверах,
      содержащий список страниц, которые не должны индексироваться роботами);
      скачанные robots.txt для каждого посещённого сервера сохраняются и при
      последующем обращении к этому серверу берутся из файла (предполагается,
      что robots.txt меняются не слишком часто)
    
Планы:

    на ближайшее будущее:
        1. автологгер, статистика, клавиатура
        2. доделать (и дотестить) request-rate и visit-time
        
    на перспективу:
        1. хранилище посещённых ссылок (bloom filter?)
        2. почитать и придумать способ хранения robots.txt
        3. формы с POST-запросами (?)

Исправить:

    1. замирание потоков
    2. несколько потоков одновременно могут скачать и сохранить один и тот же robots.txt
