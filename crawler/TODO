Что есть:

    программа скачивает страницу из сети, вынимает из неё все ссылки,
    определяет, по каким из них нужно перейти дальше, скачивает их, и т.д.
    если похоже, что ссылка ведёт на книжку, то она выводится в ответ
    
    поддерживается стандарт robots.txt (файл, содержащийся на многих серверах,
    содержащий список страниц, которые не должны индексироваться роботами);
    скачанные robots.txt для каждого посещённого сервера сохраняются и при
    последующем обращении к этому серверу берутся из файла (предполагается,
    что robots.txt меняются не слишком часто)
    
Планы:

    на ближайшее будущее:
        1. очередь с приоритетами:
            а) добавлять ссылку не если в очереди меньше XXX элементов, а если
               минимальная стоимость меньше стоимости добавляемого элемента
            б) поток должен по возможности брать ссылку на хост, ссылку на
               который не обрабатывает никакой другой поток
            в) идти больше вширь (не оставаться подолгу на одном хосте,
               ограничивать количество ссылок с одного хоста)
        
    на перспективу:
        1. почитать и придумать способ хранения robots.txt
        2. хранилище посещённых ссылок (скидывать на диск время от времени)
        ?. вывод в результат не только ссылок, заканчивающихся на .pdf, .prc, etc.
        ?. формы с POST-запросами (?)

Исправить:

