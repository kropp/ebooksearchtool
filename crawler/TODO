Что есть:

    программа скачивает страницу из сети, вынимает из неё все ссылки,
    определяет, по каким из них нужно перейти дальше, скачивает их, и т.д.
    если похоже, что ссылка ведёт на книжку, то она выводится в ответ
    - поддерживается стандарт robots.txt (файл, содержащийся на многих серверах,
      содержащий список страниц, которые не должны индексироваться роботами);
      скачанные robots.txt для каждого посещённого сервера сохраняются и при
      последующем обращении к этому серверу берутся из файла (предполагается,
      что robots.txt меняются не слишком часто)
    
Планы:

    на ближайшее будущее:
        1. логгер, статистика, клавиатура
        2. доделать взаимодействие с Analyzer'ом
        
    на перспективу:
        1. хранилище посещённых ссылок (bloom filter?)
        2. формы с POST-запросами
        3. почитать и придумать способ хранения robots.txt

Исправить:

    1. на некотором этапе краулящие потоки почему-то замирают
    2. сделать нормальное завершение работы (закрытие потоков, файлов)
    3. убрать повторяющийся код (скачивания страницы)
    4. несколько потоков одновременно могут скачать и сохранить один и тот же robots.txt
    5. иногда потоки не реагируют на interrupt
