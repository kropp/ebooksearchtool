Что есть:

	программа скачивает страницу из сети, вынимает из неё все ссылки,
	определяет, по каким из них нужно перейти дальше, скачивает их, и т.д.
	- поддерживается robots.txt (файл, содержащийся на многих серверах,
	  содержащий список страниц, которые не должны индексироваться роботами);
	  скачанные robots.txt для каждого посещённого сервера сохраняются и при
	  последующем обращении к этому серверу берутся из файла (предполагается,
	  что robots.txt меняются не слишком часто)
	
Планы:

	на ближайшее будущее:
		1. не переходить по разным ссылкам на одно и то же (www, слэш в конце и т.п.)
		2. переделать внутренность классов (перенести методы туда, где они нужны),
		   разобрать беспорядок с константами и значениями, которые можно изменить
		
	на перспективу:
		1. многопоточность
		2. хранилище посещённых ссылок
		3. вывод результатов

Баги:

	1. с www.yandex.ru не вытаскивается ни одной ссылки O_o
	   (видимо, виноват джавовский HTML-парсер)
