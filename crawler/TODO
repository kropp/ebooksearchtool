Что есть:

	программа скачивает страницу из сети, вынимает из неё все ссылки, определяет какие из них нужно скачать дальше, скачивает, и т.д.
	
Планы:

	на ближайшее будущее:
		1. кэшировать уже скачанные и разобранные robots.txt
		2. использовать встроенный в Java HTML-парсер
		3. не переходить по разным ссылкам на одно и то же (www, слэш в конце и т.п.)
		
	на перспективу:
		1. многопоточность
		2. хранилище посещённых ссылок
