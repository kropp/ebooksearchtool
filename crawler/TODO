Что есть:

    программа скачивает страницу из сети, вынимает из неё все ссылки,
    определяет, по каким из них нужно перейти дальше, скачивает их, и т.д.
    если похоже, что ссылка ведёт на книжку, то она выводится в ответ
    
    поддерживается стандарт robots.txt (файл, содержащийся на многих серверах,
    содержащий список страниц, которые не должны индексироваться роботами);
    скачанные robots.txt для каждого посещённого сервера сохраняются и при
    последующем обращении к этому серверу берутся из файла (предполагается,
    что robots.txt меняются не слишком часто)
    
Планы:

    на ближайшее будущее:
        1. HTTPS (не подключаться к недоверенным страницам)
        
    на перспективу:
        1. хранилище посещённых ссылок (скидывать на диск время от времени)
        2. почитать и придумать способ хранения robots.txt
        3. определение похожести ссылки на книжку
        4. идти больше вширь (не оставаться подолгу на одном хосте, ограничивать количество ссылок с одного хоста)
        5. формы с POST-запросами (?)

Исправить:

    1. доделать (и дотестить) request-rate и visit-time (хранить для каждого хоста время его последнего посещения)
    2. поток должен по возможности брать ссылку на хост, ссылку на который не обрабатывает никакой другой поток
