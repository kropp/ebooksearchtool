Что есть:

	программа скачивает страницу из сети, вынимает из неё все ссылки,
	определяет, по каким из них нужно перейти дальше, скачивает их, и т.д.
	поддерживается robots.txt (список страниц, которые не должны индексироваться роботами)
	
Планы:

	на ближайшее будущее:
		1. придумать нормальный способ кэшировать уже скачанные и разобранные robots.txt
		2. научиться использовать встроенный в Java HTML-парсер
		3. не переходить по разным ссылкам на одно и то же (www, слэш в конце и т.п.)
		
	на перспективу:
		1. многопоточность
		2. хранилище посещённых ссылок

Баги:

	1. в robots.txt бывают комментарии и разные маски
