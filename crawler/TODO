Что есть:

    программа скачивает страницу из сети, вынимает из неё все ссылки,
    определяет, по каким из них нужно перейти дальше, скачивает их, и т.д.
    если похоже, что ссылка ведёт на книжку, то она выводится в ответ
    
    поддерживается стандарт robots.txt (файл, содержащийся на многих серверах,
    содержащий список страниц, которые не должны индексироваться роботами);
    скачанные robots.txt для каждого посещённого сервера сохраняются и при
    последующем обращении к этому серверу берутся из файла (предполагается,
    что robots.txt меняются не слишком часто)
    
Планы:

    на ближайшее будущее:
        1. очередь с приоритетами:
            а) добавлять ссылку не если в очереди меньше XXX элементов, а если
               минимальная стоимость меньше стоимости добавляемого элемента (куча)
            б) поток должен по возможности брать ссылку на хост, ссылку на
               который не обрабатывает никакой другой поток
            в) идти больше вширь (не оставаться подолгу на одном хосте,
               ограничивать количество ссылок с одного хоста)
        
    на перспективу:
        1. почитать об и придумать структуры данных
            а) очередь ссылок
            б) хранилище посещённых ссылок
            в) robots.txt
        ?. вывод в результат не только ссылок, заканчивающихся на .pdf, .prc, etc.
        ?. формы с POST-запросами (?)

Исправить:

    1. дотестить поддержку Request-rate
    2. (после п.1) ввести глобальный Request-rate на все серверы (чтобы не скачивать
       сотню страниц в секунду с сервера, даже если у него в robots.txt не указано,
       что он этого не хочет)
