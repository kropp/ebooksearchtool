Что есть:

    программа скачивает страницу из сети, вынимает из неё все ссылки,
    определяет, по каким из них нужно перейти дальше, скачивает их, и т.д.
    если похоже, что ссылка ведёт на книжку, то она выводится в ответ
    
    поддерживается стандарт robots.txt (файл, содержащийся на многих серверах,
    содержащий список страниц, которые не должны индексироваться роботами);
    скачанные robots.txt для каждого посещённого сервера сохраняются и при
    последующем обращении к этому серверу берутся из файла (предполагается,
    что robots.txt меняются не слишком часто)
    
Планы:

    на ближайшее будущее:
        1. сделать очередь с приоритетами:
            а) поток должен по возможности брать ссылку на хост, ссылку на который не обрабатывает никакой другой поток
            б) идти больше вширь (не оставаться подолгу на одном хосте, ограничивать количество ссылок с одного хоста)
        
    на перспективу:
        1. хранилище посещённых ссылок (скидывать на диск время от времени)
        2. почитать и придумать способ хранения robots.txt
        3. определение похожести ссылки на книжку
        4. формы с POST-запросами (?)

Исправить:

