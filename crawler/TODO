Что есть:

    программа скачивает страницу из сети, вынимает из неё все ссылки,
    определяет, по каким из них нужно перейти дальше, скачивает их, и т.д.
    если похоже, что ссылка ведёт на книжку, то она выводится в ответ
    
    поддерживается стандарт robots.txt (файл, содержащийся на многих серверах,
    содержащий список страниц, которые не должны индексироваться роботами);
    скачанные robots.txt для каждого посещённого сервера сохраняются и при
    последующем обращении к этому серверу берутся из файла (предполагается,
    что robots.txt меняются не слишком часто)
    
Планы:

    на ближайшее будущее:
        1. очередь с приоритетами:
            [+] а) добавлять ссылку, если минимальная стоимость элемента в
                   очереди меньше стоимости добавляемого элемента
            [-] б) поток должен по возможности брать ссылку на хост, ссылку на
                   который не обрабатывает никакой другой поток
          [+/-] в) идти больше вширь (не оставаться подолгу на одном хосте,
                   ограничивать количество ссылок с одного хоста)
          [+/-] г) хост более приоритетен, если с него скачано уже много книжек
        
    на перспективу:
        1. почитать об и придумать структуры данных
            а) очередь ссылок
            б) хранилище посещённых ссылок
            в) robots.txt
        ?. вывод в результат не только ссылок, заканчивающихся на
           .epub, .pdf, .txt, .doc, etc. (+ на что ещё?)

Исправить:

    1. (после п.1) ввести глобальный Request-rate на все серверы (чтобы не
       скачивать сотню страниц в секунду с сервера, даже если у него в
       robots.txt не указано, что он этого не хочет)
    2. пересоединяться с аналайзером, если он упал
    3. воспроизвести memory leak, случившийся однажды на удалённом компе
       и понять, откуда он берётся
    4. найти, какие бывают таймауты, кроме Connection- и Read-,
       убрать ужасный код (придумать другой способ, в конце концов)
    5. Request-rate: переписать всё заново и затестить


Глобальные цели на будущее:

    - найти статьи и почитать про web context graph, focused crawling,
      а также смежные темы, попытаться применить как-то на практике
    - подумать о способах извлечения ссылок, кроме как из <a href="...">
    - подумать о нормализации ссылки
    - (нескоро) подумать о хранении robots.txt и вообще кешировании

