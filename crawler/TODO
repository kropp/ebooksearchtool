Что есть:

    программа скачивает страницу из сети, вынимает из неё все ссылки,
    определяет, по каким из них нужно перейти дальше, скачивает их, и т.д.
    если похоже, что ссылка ведёт на книжку, то она выводится в ответ
    - поддерживается стандарт robots.txt (файл, содержащийся на многих серверах,
      содержащий список страниц, которые не должны индексироваться роботами);
      скачанные robots.txt для каждого посещённого сервера сохраняются и при
      последующем обращении к этому серверу берутся из файла (предполагается,
      что robots.txt меняются не слишком часто)
    
Планы:

    на ближайшее будущее:
        1. HTTPS
        
    на перспективу:
        1. хранилище посещённых ссылок (bloom filter?)
        2. почитать и придумать способ хранения robots.txt
        3. определение похожести ссылки на книжку
        4. идти больше вширь (не оставаться подолгу на одном хосте)
        4. формы с POST-запросами (?)

Исправить:

    1. доделать (и дотестить) request-rate и visit-time
    2. потоки иногда надолго зависают (вынести скачивание совсем в отдельный поток?)
    3. несколько потоков одновременно могут скачать и сохранить один и тот же robots.txt
    4. поправить "вечное" ожидание ссылки из очереди
