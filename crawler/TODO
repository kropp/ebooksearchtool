Что есть:

	программа скачивает страницу из сети, вынимает из неё все ссылки,
	определяет, по каким из них нужно перейти дальше, скачивает их, и т.д.
	если похоже, что ссылка ведёт на книжку, то она выводится в ответ
	- поддерживается robots.txt (файл, содержащийся на многих серверах,
	  содержащий список страниц, которые не должны индексироваться роботами);
	  скачанные robots.txt для каждого посещённого сервера сохраняются и при
	  последующем обращении к этому серверу берутся из файла (предполагается,
	  что robots.txt меняются не слишком часто)
	
Планы:

	на ближайшее будущее:
		1. переделать внутренность классов (перенести методы туда, где они нужны),
		   разобрать беспорядок с константами и значениями, которые можно изменить,
		   скачивание файла с сети должно быть в одном месте, а не везде
		2. договориться о формате вывода найденных книжек
		3. конфигурирование (properties?)
		
	на перспективу:
		1. многопоточность
		2. хранилище посещённых ссылок (bloom filter?)
		3. формы с POST-запросами

Баги:

	1. с некоторых сайтов (www.yandex.ru, www.ietf.org) не вытаскивается ни одной ссылки
	   (видимо, виноват джавовский HTML-парсер)
