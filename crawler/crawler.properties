# [ Интернет-соединение ]
# connection_timeout -- таймаут соединения в мс
user_agent=ebooksearchtool
connection_timeout=5000
read_timeout=7000

# [ Соединение с analyzer'ом ]
# analyzer_enabled -- "true" или "false"
analyzer_enabled=false
analyzer_port=9999


# [ HTTP-прокси ]
# proxy_enabled -- "true" или "false"
#proxy_enabled=true
proxy_enabled=true
proxy_host=192.168.0.2
proxy_port=3128


# [ Краулер ]
# max_links_count -- пока посещённых ссылок меньше этого числа, bloom filter
#     (вероятностная структура данных) промахивается не слишком часто
# max_links_from_page -- столько ссылок со страницы выбираются случайным
#     образом для дальнейшего рассмотрения
# max_queue_size -- максимальный размер очереди ссылок
# threads_count -- количество потоков
# thread_timeout_for_link -- время в мс, через которое основной поток
#     спрашивает у потоков-краулеров, с какой ссылкой каждый из них работает
# thread_finish_time -- время в мс, которое даётся потоку-краулеру на
#     завершение, если основной поток обнаружил, что с последнего опроса не
#     изменилась ссылка, над которой этот поток-краулер работает. Спустя это
#     время повисший поток перезапускается
# waiting_for_access_timeout -- максимальное время в мс, которое ждём, чтобы
#     обратиться к хосту (у некоторых хостов существуют ограничения на время
#     следующего доступа в robots.txt). Если нужно ждать больше, чем это число,
#     то забываем про ссылку
#
# 0 значит бесконечность (там, где это имеет смысл)
max_links_count=3000000
max_links_from_page=1000
max_queue_size=3000
threads_count=20
thread_timeout_for_link=10000
thread_finish_time=5000
waiting_for_access_timeout=5000
# large_amount_of_books -- если с хоста найдено как минимум столько книжек,
#     он навсегда признаётся большим источником
# max_links_from_host -- максимальное количество ссылок, которое может быть
#     загружено с хоста (до очередного очищения статистики)
# max_links_from_large_source -- максимальное количество ссылок, которое может
#     быть загружено с большого источника (до очередного очищения статистики)
# max_links_from_second_level_domain -- максимальное количество ссылок, которое
#     может быть загружено с каждого домена второго уровня (таких, как
#     wikipedia.org, tumblr.com, meetup.com, и т.д.)
# host_stats_cleanup_period -- период времени в мс, через который очищается
#     статистика о том, сколько ссылок откуда было скачано (это сделано, чтобы
#     не занимать много памяти устаревающей информацией)
large_amount_of_books=1
max_links_from_host=1000
max_links_from_large_source=7500
max_links_from_second_level_domain=750
host_stats_cleanup_period=90000
# good_domains -- рассматривать хосты только с этих доменов
# good_sites -- дать хостам, в написании которых есть эти строки, наибольший
#     приоритет (пройти сайт полностью, игнорируя ограничения, заданные выше)
# bad_sites -- дать хостам, в написании которых есть эти строки, наименьший
#     приоритет (не идти туда, пока есть другие ссылки для рассмотрения)
good_domains=com net org info edu gov biz ru ua uk us
#good_domains=ru
good_sites=
bad_sites=facebook /wiki rutube endless amazon flickr blogspot wordpress google



# [ Логи ]
# debug -- ждать ввода с клавиатуры: при введённой пустой строке выводится
#     на экран и в файл информацию о каждом потоке и статистику, при
#     непустой -- завершить приложение.
# debug_file -- файл, в который эту статистику выводить
#
# Внимание! При debug=true невозможно запустить приложение
# в фоновом режиме (командой bg)!
#
# имена файлов можно оставить пустыми, чтобы отключить вывод в файл
debug=false
debug_file=dump.txt
found_books_file=found_books/found_log.xml
max_record_count=50

log_file=log.txt
log_to_screen=true

log_downloaded_robots_txt=false
log_crawled_pages=true
log_found_books=false
log_errors=true
log_misc=true
